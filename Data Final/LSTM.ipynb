{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import LSTM, Dropout, Dense\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"https://raw.githubusercontent.com/nsethi31/Kaggle-Data-Credit-Card-Fraud-Detection/master/creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "284807"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_count = len(data)\n",
    "rec_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(X, y, sequence_length = 10, step = 1):\n",
    "    X_local = []\n",
    "    y_local = []\n",
    "    for start in range(0, len(data) - sequence_length, step):\n",
    "        end = start + sequence_length\n",
    "        X_local.append(X[start:end])\n",
    "        y_local.append(y[end-1])\n",
    "    return np.array(X_local), np.array(y_local)\n",
    "\n",
    "X_sequence, y = generate_data(data.loc[:, \"V1\":\"V28\"].values, data.Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((284797, 10, 28), (284797,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sequence.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 100)               51600     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 51,701\n",
      "Trainable params: 51,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(LSTM(100, input_shape = (10, 28)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss=\"binary_crossentropy\"\n",
    "              , metrics=[keras.metrics.binary_accuracy]\n",
    "              , optimizer=\"adam\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size = int(len(X_sequence) * 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_sequence[:training_size], y[:training_size]\n",
    "X_test, y_test = X_sequence[training_size:], y[training_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199357, 10, 28)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199357,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3115/3115 [==============================] - 85s 24ms/step - loss: 0.0081 - binary_accuracy: 0.9976\n",
      "Epoch 2/50\n",
      "3115/3115 [==============================] - 75s 24ms/step - loss: 0.0036 - binary_accuracy: 0.9993\n",
      "Epoch 3/50\n",
      "3115/3115 [==============================] - 55s 18ms/step - loss: 0.0035 - binary_accuracy: 0.9993\n",
      "Epoch 4/50\n",
      "3115/3115 [==============================] - 36s 12ms/step - loss: 0.0032 - binary_accuracy: 0.9994\n",
      "Epoch 5/50\n",
      "3115/3115 [==============================] - 38s 12ms/step - loss: 0.0032 - binary_accuracy: 0.9993\n",
      "Epoch 6/50\n",
      "3115/3115 [==============================] - 40s 13ms/step - loss: 0.0032 - binary_accuracy: 0.9993\n",
      "Epoch 7/50\n",
      "3115/3115 [==============================] - 39s 12ms/step - loss: 0.0029 - binary_accuracy: 0.9993\n",
      "Epoch 8/50\n",
      "3115/3115 [==============================] - 112s 36ms/step - loss: 0.0026 - binary_accuracy: 0.9994\n",
      "Epoch 9/50\n",
      "3115/3115 [==============================] - 85s 27ms/step - loss: 0.0024 - binary_accuracy: 0.9994\n",
      "Epoch 10/50\n",
      "3115/3115 [==============================] - 84s 27ms/step - loss: 0.0021 - binary_accuracy: 0.9995\n",
      "Epoch 11/50\n",
      "3115/3115 [==============================] - 53s 17ms/step - loss: 0.0019 - binary_accuracy: 0.9995\n",
      "Epoch 12/50\n",
      "3115/3115 [==============================] - 47s 15ms/step - loss: 0.0016 - binary_accuracy: 0.9995\n",
      "Epoch 13/50\n",
      "3115/3115 [==============================] - 52s 17ms/step - loss: 0.0013 - binary_accuracy: 0.9996\n",
      "Epoch 14/50\n",
      "3115/3115 [==============================] - 61s 20ms/step - loss: 9.1871e-04 - binary_accuracy: 0.9997\n",
      "Epoch 15/50\n",
      "3115/3115 [==============================] - 63s 20ms/step - loss: 7.6432e-04 - binary_accuracy: 0.9997\n",
      "Epoch 16/50\n",
      "3115/3115 [==============================] - 64s 20ms/step - loss: 6.3714e-04 - binary_accuracy: 0.9998\n",
      "Epoch 17/50\n",
      "3115/3115 [==============================] - 62s 20ms/step - loss: 3.9818e-04 - binary_accuracy: 0.9999\n",
      "Epoch 18/50\n",
      "3115/3115 [==============================] - 60s 19ms/step - loss: 3.9634e-04 - binary_accuracy: 0.9999\n",
      "Epoch 19/50\n",
      "3115/3115 [==============================] - 54s 17ms/step - loss: 3.5465e-04 - binary_accuracy: 0.9999\n",
      "Epoch 20/50\n",
      "3115/3115 [==============================] - 43s 14ms/step - loss: 2.9827e-04 - binary_accuracy: 0.9999\n",
      "Epoch 21/50\n",
      "3115/3115 [==============================] - 43s 14ms/step - loss: 3.6177e-04 - binary_accuracy: 0.9999\n",
      "Epoch 22/50\n",
      "3115/3115 [==============================] - 61s 20ms/step - loss: 2.0244e-04 - binary_accuracy: 0.9999\n",
      "Epoch 23/50\n",
      "3115/3115 [==============================] - 49s 16ms/step - loss: 2.3930e-04 - binary_accuracy: 0.9999\n",
      "Epoch 24/50\n",
      "3115/3115 [==============================] - 44s 14ms/step - loss: 3.0565e-04 - binary_accuracy: 0.9999\n",
      "Epoch 25/50\n",
      "3115/3115 [==============================] - 45s 14ms/step - loss: 2.5787e-04 - binary_accuracy: 0.9999\n",
      "Epoch 26/50\n",
      "3115/3115 [==============================] - 37s 12ms/step - loss: 8.0880e-05 - binary_accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "3115/3115 [==============================] - 31s 10ms/step - loss: 1.6785e-04 - binary_accuracy: 0.9999\n",
      "Epoch 28/50\n",
      "3115/3115 [==============================] - 30s 10ms/step - loss: 1.6786e-04 - binary_accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "3115/3115 [==============================] - 34s 11ms/step - loss: 1.1911e-04 - binary_accuracy: 0.9999\n",
      "Epoch 30/50\n",
      "3115/3115 [==============================] - 33s 11ms/step - loss: 1.6413e-04 - binary_accuracy: 0.9999\n",
      "Epoch 31/50\n",
      "3115/3115 [==============================] - 31s 10ms/step - loss: 3.1342e-05 - binary_accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "3115/3115 [==============================] - 30s 10ms/step - loss: 1.0887e-04 - binary_accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "3115/3115 [==============================] - 30s 10ms/step - loss: 1.4426e-04 - binary_accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "3115/3115 [==============================] - 28s 9ms/step - loss: 9.3788e-05 - binary_accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "3115/3115 [==============================] - 29s 9ms/step - loss: 4.9393e-05 - binary_accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "3115/3115 [==============================] - 28s 9ms/step - loss: 2.7294e-04 - binary_accuracy: 0.9999\n",
      "Epoch 37/50\n",
      "3115/3115 [==============================] - 28s 9ms/step - loss: 8.8154e-05 - binary_accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "3115/3115 [==============================] - 30s 10ms/step - loss: 3.8937e-05 - binary_accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "3115/3115 [==============================] - 104s 33ms/step - loss: 1.1460e-04 - binary_accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "3115/3115 [==============================] - 132s 42ms/step - loss: 1.0229e-04 - binary_accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "3115/3115 [==============================] - 216s 69ms/step - loss: 7.8465e-05 - binary_accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "3115/3115 [==============================] - 243s 78ms/step - loss: 1.3875e-04 - binary_accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "3115/3115 [==============================] - 329s 106ms/step - loss: 7.2604e-05 - binary_accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "3115/3115 [==============================] - 334s 107ms/step - loss: 3.6697e-05 - binary_accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "3115/3115 [==============================] - 579s 186ms/step - loss: 1.0265e-04 - binary_accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "3115/3115 [==============================] - 433s 139ms/step - loss: 2.2832e-05 - binary_accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "3115/3115 [==============================] - 447s 143ms/step - loss: 3.6928e-06 - binary_accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "3115/3115 [==============================] - 459s 148ms/step - loss: 1.9601e-04 - binary_accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "3115/3115 [==============================] - 261s 84ms/step - loss: 7.3935e-05 - binary_accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "3115/3115 [==============================] - 101s 33ms/step - loss: 5.9322e-05 - binary_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b9b8241940>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=64, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2670/2670 [==============================] - 16s 5ms/step - loss: 0.0090 - binary_accuracy: 0.9995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.00903936754912138, 0.9995201230049133]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2670/2670 [==============================] - 15s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "y_test_prob = model.predict(X_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = np.where(y_test_prob > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[85328,     4],\n",
       "       [   37,    71]], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruebas Nuevas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   var1(t-1)  var2(t-1)  var3(t-1)  var4(t-1)  var5(t-1)  var6(t-1)  \\\n",
      "1   0.129779   0.352941   0.245902   0.527273   0.666667   0.002290   \n",
      "2   0.148893   0.367647   0.245902   0.527273   0.666667   0.003811   \n",
      "3   0.159960   0.426471   0.229508   0.545454   0.666667   0.005332   \n",
      "4   0.182093   0.485294   0.229508   0.563637   0.666667   0.008391   \n",
      "5   0.138833   0.485294   0.229508   0.563637   0.666667   0.009912   \n",
      "\n",
      "   var7(t-1)  var8(t-1)   var1(t)  \n",
      "1   0.000000        0.0  0.148893  \n",
      "2   0.000000        0.0  0.159960  \n",
      "3   0.000000        0.0  0.182093  \n",
      "4   0.037037        0.0  0.138833  \n",
      "5   0.074074        0.0  0.109658  \n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    " \n",
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg\n",
    " \n",
    "# load dataset\n",
    "dataset = read_csv('pollution.csv', header=0, index_col=0)\n",
    "values = dataset.values\n",
    "# integer encode direction\n",
    "encoder = LabelEncoder()\n",
    "values[:,4] = encoder.fit_transform(values[:,4])\n",
    "# ensure all data is float\n",
    "values = values.astype('float32')\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, 1, 1)\n",
    "# drop columns we don't want to predict\n",
    "reframed.drop(reframed.columns[[9,10,11,12,13,14,15]], axis=1, inplace=True)\n",
    "print(reframed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 43800 entries, 2010-01-02 00:00:00 to 2014-12-31 23:00:00\n",
      "Data columns (total 8 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   pollution  43800 non-null  float64\n",
      " 1   dew        43800 non-null  int64  \n",
      " 2   temp       43800 non-null  float64\n",
      " 3   press      43800 non-null  float64\n",
      " 4   wnd_dir    43800 non-null  object \n",
      " 5   wnd_spd    43800 non-null  float64\n",
      " 6   snow       43800 non-null  int64  \n",
      " 7   rain       43800 non-null  int64  \n",
      "dtypes: float64(4), int64(3), object(1)\n",
      "memory usage: 3.0+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8760, 1, 8) (8760,) (35039, 1, 8) (35039,)\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "values = reframed.values\n",
    "n_train_hours = 365 * 24\n",
    "train = values[:n_train_hours, :]\n",
    "test = values[n_train_hours:, :]\n",
    "# split into input and outputs\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pollution</th>\n",
       "      <th>dew</th>\n",
       "      <th>temp</th>\n",
       "      <th>press</th>\n",
       "      <th>wnd_dir</th>\n",
       "      <th>wnd_spd</th>\n",
       "      <th>snow</th>\n",
       "      <th>rain</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-02 00:00:00</th>\n",
       "      <td>129.0</td>\n",
       "      <td>-16</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>SE</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-02 01:00:00</th>\n",
       "      <td>148.0</td>\n",
       "      <td>-15</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>SE</td>\n",
       "      <td>2.68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-02 02:00:00</th>\n",
       "      <td>159.0</td>\n",
       "      <td>-11</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>1021.0</td>\n",
       "      <td>SE</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-02 03:00:00</th>\n",
       "      <td>181.0</td>\n",
       "      <td>-7</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>1022.0</td>\n",
       "      <td>SE</td>\n",
       "      <td>5.36</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-02 04:00:00</th>\n",
       "      <td>138.0</td>\n",
       "      <td>-7</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>1022.0</td>\n",
       "      <td>SE</td>\n",
       "      <td>6.25</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 19:00:00</th>\n",
       "      <td>8.0</td>\n",
       "      <td>-23</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1034.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>231.97</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 20:00:00</th>\n",
       "      <td>10.0</td>\n",
       "      <td>-22</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>1034.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>237.78</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 21:00:00</th>\n",
       "      <td>10.0</td>\n",
       "      <td>-22</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>1034.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>242.70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 22:00:00</th>\n",
       "      <td>8.0</td>\n",
       "      <td>-22</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>1034.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>246.72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 23:00:00</th>\n",
       "      <td>12.0</td>\n",
       "      <td>-21</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>1034.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>249.85</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43800 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     pollution  dew  temp   press wnd_dir  wnd_spd  snow  rain\n",
       "date                                                                          \n",
       "2010-01-02 00:00:00      129.0  -16  -4.0  1020.0      SE     1.79     0     0\n",
       "2010-01-02 01:00:00      148.0  -15  -4.0  1020.0      SE     2.68     0     0\n",
       "2010-01-02 02:00:00      159.0  -11  -5.0  1021.0      SE     3.57     0     0\n",
       "2010-01-02 03:00:00      181.0   -7  -5.0  1022.0      SE     5.36     1     0\n",
       "2010-01-02 04:00:00      138.0   -7  -5.0  1022.0      SE     6.25     2     0\n",
       "...                        ...  ...   ...     ...     ...      ...   ...   ...\n",
       "2014-12-31 19:00:00        8.0  -23  -2.0  1034.0      NW   231.97     0     0\n",
       "2014-12-31 20:00:00       10.0  -22  -3.0  1034.0      NW   237.78     0     0\n",
       "2014-12-31 21:00:00       10.0  -22  -3.0  1034.0      NW   242.70     0     0\n",
       "2014-12-31 22:00:00        8.0  -22  -4.0  1034.0      NW   246.72     0     0\n",
       "2014-12-31 23:00:00       12.0  -21  -3.0  1034.0      NW   249.85     0     0\n",
       "\n",
       "[43800 rows x 8 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   var1(t-1)  var2(t-1)  var3(t-1)  var4(t-1)  var5(t-1)  var6(t-1)  \\\n",
      "1   0.129779   0.352941   0.245902   0.527273   0.666667   0.002290   \n",
      "2   0.148893   0.367647   0.245902   0.527273   0.666667   0.003811   \n",
      "3   0.159960   0.426471   0.229508   0.545454   0.666667   0.005332   \n",
      "4   0.182093   0.485294   0.229508   0.563637   0.666667   0.008391   \n",
      "5   0.138833   0.485294   0.229508   0.563637   0.666667   0.009912   \n",
      "\n",
      "   var7(t-1)  var8(t-1)   var1(t)  \n",
      "1   0.000000        0.0  0.148893  \n",
      "2   0.000000        0.0  0.159960  \n",
      "3   0.000000        0.0  0.182093  \n",
      "4   0.037037        0.0  0.138833  \n",
      "5   0.074074        0.0  0.109658  \n",
      "(8760, 1, 8) (8760,) (35039, 1, 8) (35039,)\n",
      "Epoch 1/50\n",
      "122/122 - 4s - loss: 0.0613 - val_loss: 0.0559 - 4s/epoch - 30ms/step\n",
      "Epoch 2/50\n",
      "122/122 - 1s - loss: 0.0403 - val_loss: 0.0653 - 672ms/epoch - 6ms/step\n",
      "Epoch 3/50\n",
      "122/122 - 1s - loss: 0.0258 - val_loss: 0.0544 - 657ms/epoch - 5ms/step\n",
      "Epoch 4/50\n",
      "122/122 - 1s - loss: 0.0196 - val_loss: 0.0466 - 666ms/epoch - 5ms/step\n",
      "Epoch 5/50\n",
      "122/122 - 1s - loss: 0.0172 - val_loss: 0.0377 - 669ms/epoch - 5ms/step\n",
      "Epoch 6/50\n",
      "122/122 - 1s - loss: 0.0158 - val_loss: 0.0268 - 713ms/epoch - 6ms/step\n",
      "Epoch 7/50\n",
      "122/122 - 1s - loss: 0.0151 - val_loss: 0.0192 - 647ms/epoch - 5ms/step\n",
      "Epoch 8/50\n",
      "122/122 - 1s - loss: 0.0149 - val_loss: 0.0173 - 654ms/epoch - 5ms/step\n",
      "Epoch 9/50\n",
      "122/122 - 1s - loss: 0.0149 - val_loss: 0.0164 - 587ms/epoch - 5ms/step\n",
      "Epoch 10/50\n",
      "122/122 - 1s - loss: 0.0147 - val_loss: 0.0154 - 599ms/epoch - 5ms/step\n",
      "Epoch 11/50\n",
      "122/122 - 1s - loss: 0.0147 - val_loss: 0.0152 - 631ms/epoch - 5ms/step\n",
      "Epoch 12/50\n",
      "122/122 - 1s - loss: 0.0147 - val_loss: 0.0146 - 621ms/epoch - 5ms/step\n",
      "Epoch 13/50\n",
      "122/122 - 1s - loss: 0.0147 - val_loss: 0.0148 - 613ms/epoch - 5ms/step\n",
      "Epoch 14/50\n",
      "122/122 - 1s - loss: 0.0148 - val_loss: 0.0146 - 636ms/epoch - 5ms/step\n",
      "Epoch 15/50\n",
      "122/122 - 1s - loss: 0.0145 - val_loss: 0.0143 - 624ms/epoch - 5ms/step\n",
      "Epoch 16/50\n",
      "122/122 - 1s - loss: 0.0146 - val_loss: 0.0140 - 645ms/epoch - 5ms/step\n",
      "Epoch 17/50\n",
      "122/122 - 1s - loss: 0.0146 - val_loss: 0.0139 - 663ms/epoch - 5ms/step\n",
      "Epoch 18/50\n",
      "122/122 - 1s - loss: 0.0145 - val_loss: 0.0139 - 654ms/epoch - 5ms/step\n",
      "Epoch 19/50\n",
      "122/122 - 1s - loss: 0.0145 - val_loss: 0.0138 - 647ms/epoch - 5ms/step\n",
      "Epoch 20/50\n",
      "122/122 - 1s - loss: 0.0145 - val_loss: 0.0137 - 742ms/epoch - 6ms/step\n",
      "Epoch 21/50\n",
      "122/122 - 1s - loss: 0.0145 - val_loss: 0.0137 - 702ms/epoch - 6ms/step\n",
      "Epoch 22/50\n",
      "122/122 - 1s - loss: 0.0144 - val_loss: 0.0137 - 639ms/epoch - 5ms/step\n",
      "Epoch 23/50\n",
      "122/122 - 1s - loss: 0.0145 - val_loss: 0.0137 - 640ms/epoch - 5ms/step\n",
      "Epoch 24/50\n",
      "122/122 - 1s - loss: 0.0145 - val_loss: 0.0137 - 660ms/epoch - 5ms/step\n",
      "Epoch 25/50\n",
      "122/122 - 1s - loss: 0.0144 - val_loss: 0.0138 - 599ms/epoch - 5ms/step\n",
      "Epoch 26/50\n",
      "122/122 - 1s - loss: 0.0144 - val_loss: 0.0137 - 615ms/epoch - 5ms/step\n",
      "Epoch 27/50\n",
      "122/122 - 1s - loss: 0.0144 - val_loss: 0.0137 - 609ms/epoch - 5ms/step\n",
      "Epoch 28/50\n",
      "122/122 - 1s - loss: 0.0144 - val_loss: 0.0137 - 632ms/epoch - 5ms/step\n",
      "Epoch 29/50\n",
      "122/122 - 1s - loss: 0.0145 - val_loss: 0.0138 - 596ms/epoch - 5ms/step\n",
      "Epoch 30/50\n",
      "122/122 - 1s - loss: 0.0145 - val_loss: 0.0138 - 607ms/epoch - 5ms/step\n",
      "Epoch 31/50\n",
      "122/122 - 1s - loss: 0.0144 - val_loss: 0.0137 - 653ms/epoch - 5ms/step\n",
      "Epoch 32/50\n",
      "122/122 - 1s - loss: 0.0144 - val_loss: 0.0137 - 632ms/epoch - 5ms/step\n",
      "Epoch 33/50\n",
      "122/122 - 1s - loss: 0.0144 - val_loss: 0.0136 - 654ms/epoch - 5ms/step\n",
      "Epoch 34/50\n",
      "122/122 - 1s - loss: 0.0145 - val_loss: 0.0136 - 655ms/epoch - 5ms/step\n",
      "Epoch 35/50\n",
      "122/122 - 1s - loss: 0.0145 - val_loss: 0.0136 - 651ms/epoch - 5ms/step\n",
      "Epoch 36/50\n",
      "122/122 - 1s - loss: 0.0144 - val_loss: 0.0136 - 694ms/epoch - 6ms/step\n",
      "Epoch 37/50\n",
      "122/122 - 1s - loss: 0.0144 - val_loss: 0.0136 - 697ms/epoch - 6ms/step\n",
      "Epoch 38/50\n",
      "122/122 - 1s - loss: 0.0144 - val_loss: 0.0136 - 612ms/epoch - 5ms/step\n",
      "Epoch 39/50\n",
      "122/122 - 1s - loss: 0.0146 - val_loss: 0.0137 - 617ms/epoch - 5ms/step\n",
      "Epoch 40/50\n",
      "122/122 - 1s - loss: 0.0146 - val_loss: 0.0137 - 594ms/epoch - 5ms/step\n",
      "Epoch 41/50\n",
      "122/122 - 1s - loss: 0.0144 - val_loss: 0.0136 - 629ms/epoch - 5ms/step\n",
      "Epoch 42/50\n",
      "122/122 - 1s - loss: 0.0144 - val_loss: 0.0135 - 616ms/epoch - 5ms/step\n",
      "Epoch 43/50\n",
      "122/122 - 1s - loss: 0.0144 - val_loss: 0.0136 - 628ms/epoch - 5ms/step\n",
      "Epoch 44/50\n",
      "122/122 - 1s - loss: 0.0144 - val_loss: 0.0136 - 734ms/epoch - 6ms/step\n",
      "Epoch 45/50\n",
      "122/122 - 1s - loss: 0.0143 - val_loss: 0.0136 - 643ms/epoch - 5ms/step\n",
      "Epoch 46/50\n",
      "122/122 - 1s - loss: 0.0144 - val_loss: 0.0135 - 605ms/epoch - 5ms/step\n",
      "Epoch 47/50\n",
      "122/122 - 1s - loss: 0.0143 - val_loss: 0.0136 - 598ms/epoch - 5ms/step\n",
      "Epoch 48/50\n",
      "122/122 - 1s - loss: 0.0144 - val_loss: 0.0136 - 645ms/epoch - 5ms/step\n",
      "Epoch 49/50\n",
      "122/122 - 1s - loss: 0.0143 - val_loss: 0.0137 - 597ms/epoch - 5ms/step\n",
      "Epoch 50/50\n",
      "122/122 - 1s - loss: 0.0144 - val_loss: 0.0136 - 610ms/epoch - 5ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 26.598\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    " \n",
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg\n",
    " \n",
    "# load dataset\n",
    "dataset = read_csv('pollution.csv', header=0, index_col=0)\n",
    "values = dataset.values\n",
    "# integer encode direction\n",
    "encoder = LabelEncoder()\n",
    "values[:,4] = encoder.fit_transform(values[:,4])\n",
    "# ensure all data is float\n",
    "values = values.astype('float32')\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, 1, 1)\n",
    "# drop columns we don't want to predict\n",
    "reframed.drop(reframed.columns[[9,10,11,12,13,14,15]], axis=1, inplace=True)\n",
    "print(reframed.head())\n",
    " \n",
    "# split into train and test sets\n",
    "values = reframed.values\n",
    "n_train_hours = 365 * 24\n",
    "train = values[:n_train_hours, :]\n",
    "test = values[n_train_hours:, :]\n",
    "# split into input and outputs\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    " \n",
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    " \n",
    "# make a prediction\n",
    "yhat = model.predict(test_X)\n",
    "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "# invert scaling for forecast\n",
    "inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]\n",
    "# calculate RMSE\n",
    "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    " \n",
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg\n",
    " \n",
    "# load dataset\n",
    "dataset = read_csv('pollution.csv', header=0, index_col=0)\n",
    "values = dataset.values\n",
    "# integer encode direction\n",
    "encoder = LabelEncoder()\n",
    "values[:,4] = encoder.fit_transform(values[:,4])\n",
    "# ensure all data is float\n",
    "values = values.astype('float32')\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1(t-1)</th>\n",
       "      <th>var2(t-1)</th>\n",
       "      <th>var3(t-1)</th>\n",
       "      <th>var4(t-1)</th>\n",
       "      <th>var5(t-1)</th>\n",
       "      <th>var6(t-1)</th>\n",
       "      <th>var7(t-1)</th>\n",
       "      <th>var8(t-1)</th>\n",
       "      <th>var1(t)</th>\n",
       "      <th>var2(t)</th>\n",
       "      <th>var3(t)</th>\n",
       "      <th>var4(t)</th>\n",
       "      <th>var5(t)</th>\n",
       "      <th>var6(t)</th>\n",
       "      <th>var7(t)</th>\n",
       "      <th>var8(t)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.129779</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.245902</td>\n",
       "      <td>0.527273</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.002290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.148893</td>\n",
       "      <td>0.367647</td>\n",
       "      <td>0.245902</td>\n",
       "      <td>0.527273</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.003811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.148893</td>\n",
       "      <td>0.367647</td>\n",
       "      <td>0.245902</td>\n",
       "      <td>0.527273</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.003811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.159960</td>\n",
       "      <td>0.426471</td>\n",
       "      <td>0.229508</td>\n",
       "      <td>0.545454</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.005332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.159960</td>\n",
       "      <td>0.426471</td>\n",
       "      <td>0.229508</td>\n",
       "      <td>0.545454</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.005332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.182093</td>\n",
       "      <td>0.485294</td>\n",
       "      <td>0.229508</td>\n",
       "      <td>0.563637</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.008391</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.182093</td>\n",
       "      <td>0.485294</td>\n",
       "      <td>0.229508</td>\n",
       "      <td>0.563637</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.008391</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.138833</td>\n",
       "      <td>0.485294</td>\n",
       "      <td>0.229508</td>\n",
       "      <td>0.563637</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.009912</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.138833</td>\n",
       "      <td>0.485294</td>\n",
       "      <td>0.229508</td>\n",
       "      <td>0.563637</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.009912</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.109658</td>\n",
       "      <td>0.485294</td>\n",
       "      <td>0.213115</td>\n",
       "      <td>0.563637</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.011433</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43795</th>\n",
       "      <td>0.010060</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.278689</td>\n",
       "      <td>0.763638</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.385730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008048</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.278689</td>\n",
       "      <td>0.781818</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.395659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43796</th>\n",
       "      <td>0.008048</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.278689</td>\n",
       "      <td>0.781818</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.395659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010060</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.781818</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.405588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43797</th>\n",
       "      <td>0.010060</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.781818</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.405588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010060</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.781818</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.413996</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43798</th>\n",
       "      <td>0.010060</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.781818</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.413996</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008048</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.245902</td>\n",
       "      <td>0.781818</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.420866</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43799</th>\n",
       "      <td>0.008048</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.245902</td>\n",
       "      <td>0.781818</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.420866</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012072</td>\n",
       "      <td>0.279412</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.781818</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.426216</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43799 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       var1(t-1)  var2(t-1)  var3(t-1)  var4(t-1)  var5(t-1)  var6(t-1)  \\\n",
       "1       0.129779   0.352941   0.245902   0.527273   0.666667   0.002290   \n",
       "2       0.148893   0.367647   0.245902   0.527273   0.666667   0.003811   \n",
       "3       0.159960   0.426471   0.229508   0.545454   0.666667   0.005332   \n",
       "4       0.182093   0.485294   0.229508   0.563637   0.666667   0.008391   \n",
       "5       0.138833   0.485294   0.229508   0.563637   0.666667   0.009912   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "43795   0.010060   0.264706   0.278689   0.763638   0.333333   0.385730   \n",
       "43796   0.008048   0.250000   0.278689   0.781818   0.333333   0.395659   \n",
       "43797   0.010060   0.264706   0.262295   0.781818   0.333333   0.405588   \n",
       "43798   0.010060   0.264706   0.262295   0.781818   0.333333   0.413996   \n",
       "43799   0.008048   0.264706   0.245902   0.781818   0.333333   0.420866   \n",
       "\n",
       "       var7(t-1)  var8(t-1)   var1(t)   var2(t)   var3(t)   var4(t)   var5(t)  \\\n",
       "1       0.000000        0.0  0.148893  0.367647  0.245902  0.527273  0.666667   \n",
       "2       0.000000        0.0  0.159960  0.426471  0.229508  0.545454  0.666667   \n",
       "3       0.000000        0.0  0.182093  0.485294  0.229508  0.563637  0.666667   \n",
       "4       0.037037        0.0  0.138833  0.485294  0.229508  0.563637  0.666667   \n",
       "5       0.074074        0.0  0.109658  0.485294  0.213115  0.563637  0.666667   \n",
       "...          ...        ...       ...       ...       ...       ...       ...   \n",
       "43795   0.000000        0.0  0.008048  0.250000  0.278689  0.781818  0.333333   \n",
       "43796   0.000000        0.0  0.010060  0.264706  0.262295  0.781818  0.333333   \n",
       "43797   0.000000        0.0  0.010060  0.264706  0.262295  0.781818  0.333333   \n",
       "43798   0.000000        0.0  0.008048  0.264706  0.245902  0.781818  0.333333   \n",
       "43799   0.000000        0.0  0.012072  0.279412  0.262295  0.781818  0.333333   \n",
       "\n",
       "        var6(t)   var7(t)  var8(t)  \n",
       "1      0.003811  0.000000      0.0  \n",
       "2      0.005332  0.000000      0.0  \n",
       "3      0.008391  0.037037      0.0  \n",
       "4      0.009912  0.074074      0.0  \n",
       "5      0.011433  0.111111      0.0  \n",
       "...         ...       ...      ...  \n",
       "43795  0.395659  0.000000      0.0  \n",
       "43796  0.405588  0.000000      0.0  \n",
       "43797  0.413996  0.000000      0.0  \n",
       "43798  0.420866  0.000000      0.0  \n",
       "43799  0.426216  0.000000      0.0  \n",
       "\n",
       "[43799 rows x 16 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reframed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7bdadbcc745afc9ac1e1714124e1bb5770ee7d86d26de672bb194e9104174d2a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
